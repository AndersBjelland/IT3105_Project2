{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import games\n",
    "from src import actor\n",
    "from src import interactive \n",
    "from src import tournament\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from random import choices\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "from ipywidgets import interact\n",
    "\n",
    "import self_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(x, filters, kernel_size = (3, 3), stride=1, activation=tf.nn.swish):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=kernel_size, strides=(stride, stride))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(x, filters = (256, 256), kernel_size=(3, 3), stride=1, activation = tf.nn.swish):\n",
    "    skip = x\n",
    "    \n",
    "    f1, f2 = filters\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(f1, kernel_size=kernel_size, strides=(stride, stride), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(f2, kernel_size=kernel_size, strides=(stride, stride), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Add()([x, skip])\n",
    "    x = activation(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def body(x, residual_blocks = 1, convolutional_filter = 256, residual_filters = (256, 256)):\n",
    "    x = convolutional_block(x, convolutional_filter)\n",
    "    \n",
    "    for _ in range(residual_blocks):\n",
    "        x = residual_block(x, residual_filters)\n",
    "        \n",
    "    return x\n",
    "    \n",
    "\n",
    "def policy_head(x, size, activation=tf.nn.swish, softmax_policy=False):\n",
    "    x = tf.keras.layers.Conv2D(2, kernel_size=(1, 1), strides=(1, 1))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    if softmax_policy:\n",
    "        x = tf.keras.layers.Dense(size * size)(x)\n",
    "        x = tf.keras.layers.Softmax(name='policy')(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Dense(size * size, name='policy')(x)\n",
    "    return x\n",
    "\n",
    "def value_head(x, activation=tf.nn.swish):\n",
    "    x = tf.keras.layers.Conv2D(1, kernel_size=(1, 1), strides=(1, 1))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(256)(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = tf.keras.layers.Activation(tf.nn.tanh, name='value')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def oht_model(size, residual_blocks=1, residual_filters = (256, 256), convolutional_filter=256, softmax_policy=False):\n",
    "    input = tf.keras.layers.Input(shape=(size, size, 2), name='input')\n",
    "    \n",
    "    b = body(input, residual_blocks=residual_blocks, convolutional_filter=convolutional_filter, residual_filters=residual_filters)\n",
    "    \n",
    "    policy = policy_head(b, size, softmax_policy=softmax_policy)\n",
    "    value = value_head(b)\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[policy, value])\n",
    "\n",
    "def new_model(size, body, policy=None, value=None):\n",
    "    input = tf.keras.layers.Input(shape=(size, size, 2), name='input')\n",
    "    \n",
    "    x = body(input)\n",
    "    policy = tf.keras.layers.Layer() if policy is None else policy\n",
    "    value = tf.keras.layers.Layer() if policy is None else value\n",
    "    \n",
    "    policy = tf.keras.layers.Layer(name='policy')(policy(x))\n",
    "    value = tf.keras.layers.Layer(name='value')(value(x))\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[policy, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(bitboard, player, max_size=11):\n",
    "    for i in range(max_size*max_size):\n",
    "        bit = bitboard & (0b1 << i)\n",
    "        \n",
    "        x = i % max_size\n",
    "        y = i // max_size\n",
    "        \n",
    "        if bit != 0: \n",
    "            yield (x, y) if player == 1 else (y, x)\n",
    "            \n",
    "def denormalize_noflip(encoded, size):\n",
    "    current, opponent = encoded[:, :, 0], encoded[:, :, 1]\n",
    "    game = games.Hex(size = size)\n",
    "    game.next_state = \"Disabled\"\n",
    "    for x in range(size):\n",
    "        for y in range(size):\n",
    "            if current[x, y] == 1.0:\n",
    "                game.grid[y][x] = 1\n",
    "            elif opponent[x, y] == 1.0:\n",
    "                game.grid[y][x] = 2\n",
    "    return game\n",
    "\n",
    "            \n",
    "        \n",
    "def interleave(it0, it1):\n",
    "    it0 = iter(it0)\n",
    "    it1 = iter(it1)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            yield next(it0)\n",
    "            yield next(it1)\n",
    "        except StopIteration:\n",
    "            yield from it0\n",
    "            yield from it1\n",
    "            break\n",
    "            \n",
    "            \n",
    "def decode_bitboard(player1, player2, size, max_size=11):\n",
    "    p1 = decode(player1, 1, max_size=max_size)\n",
    "    p2 = decode(player2, 2, max_size=max_size)\n",
    "    \n",
    "    game = games.Hex(size=size)\n",
    "    \n",
    "    for action in interleave(p1, p2):\n",
    "        game = game.next_state(action)\n",
    "        \n",
    "    return game\n",
    "\n",
    "def from_oht_state(state):\n",
    "    \"\"\"Converts from the OHT state representation to ours. Note that while the OHT representation allows\n",
    "    player 2 as the starting player, we DO NOT. Thus, we will always set player 1 as the starting player\"\"\"\n",
    "    player, *board = state\n",
    "    size = int(len(board)**0.5)\n",
    "    \n",
    "    assert len(board) == size * size\n",
    "    \n",
    "    # If we see an even number of pieces it means we started, if we see an odd number we were second\n",
    "    current_started = sum(1 for x in board if x != 0) % 2 == 0\n",
    "    flip = player == 1 and not current_started or player == 2 and current_started\n",
    "    \n",
    "    # Numpy makes it all simpler\n",
    "    board = np.array(board).reshape((size, size))\n",
    "    \n",
    "    # Okay, so if we are player one, and we started, then we're all set.\n",
    "    \n",
    "    # If we're player one but did not start, then we're effectively player 2 in our representation\n",
    "    # As such, we need to replace 1s with 2s and transpose the board.\n",
    "    # By the same logic, we can find that we must do the same if we started as player 2, since \n",
    "    # is really player 1 in our representation\n",
    "    if flip:\n",
    "        board = board.T\n",
    "        ones = board == 1\n",
    "        twos = board == 2\n",
    "        board[ones] = 2\n",
    "        board[twos] = 1\n",
    "        \n",
    "    game = games.Hex(size=size)\n",
    "    game.grid = [[x for x in row] for row in board]\n",
    "    game.current_player = 1 if current_started else 2\n",
    "    \n",
    "    return (not flip, game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START OF ACTUAL USEFUL STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 5\n",
    "model_name = f'{size}x{size}x2'\n",
    "\n",
    "model = new_model(\n",
    "    size=size,\n",
    "    # This is the main body\n",
    "    body=tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(15),\n",
    "        #tf.keras.layers.Dense(30)\n",
    "    ]),\n",
    "    # The policy head\n",
    "    policy=tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(size * size, activation='relu'),\n",
    "        tf.keras.layers.Softmax(),\n",
    "    ]),\n",
    "    # The value head\n",
    "    value=tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='tanh'),\n",
    "    ])\n",
    ")\n",
    "\n",
    "model.save(f'{model_name}-v0')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from train import train\n",
    "\n",
    "def delete_samples_preceeding(number):\n",
    "    for filename in os.listdir(SAMPLES_DIR):\n",
    "        filenumber, *_ = filename.split('.')\n",
    "        try:\n",
    "            if int(filenumber) < number:\n",
    "                pass\n",
    "                ! rm -r -f {SAMPLES_DIR}/{filename}\n",
    "        except Exception as e:\n",
    "            print(f'could not delete file {filename}: {e}')\n",
    "\n",
    "BASE_DIR = '.'\n",
    "SAMPLES_DIR = './samples'\n",
    "\n",
    "# The name of the model. All saved models will be named {model_name}-v{version_number}\n",
    "model_name = model_name\n",
    "# The number of simulations to perform\n",
    "simulations = 100\n",
    "# How many games to run concurrently when generating samples\n",
    "concurrents = 64\n",
    "# What kind of leaf evaluation to utilize. Either 'value_fn' or 'rollout'\n",
    "leaf_evaluation = 'value_fn'\n",
    "# What kind of encoder to use\n",
    "encoder = 'normalized'\n",
    "# We will fit the model to the last `max_sample_sets` sample sets generated\n",
    "max_sample_sets = 5\n",
    "# How many samples to generate.\n",
    "samples = concurrents * 14\n",
    "# How many 'epochs' to run. I.e. how many loops of (1) generate samples, (2) fit to rbuf to do.\n",
    "M = 5\n",
    "\n",
    "# Parameters related to training the model.\n",
    "optimizer = tf.keras.optimizers.SGD(lr=0.02)\n",
    "epochs_per_step = 10\n",
    "\n",
    "# The number of sample sets generated\n",
    "sample_sets_generated = 0\n",
    "\n",
    "for current_version in range(0, M):\n",
    "    current_model_path = f'{BASE_DIR}/{model_name}-v{current_version}'\n",
    "    sample_set_path = f'{SAMPLES_DIR}/{sample_sets_generated}.json'\n",
    "    print(f'using model {current_model_path}')\n",
    "    ! {sys.executable} generate_samples.py --samples {samples}  --simulations {simulations} --model {current_model_path} --out {sample_set_path} --size {size} --concurrents {concurrents} --evaluation {leaf_evaluation} --encoder {encoder}\n",
    "    sample_sets_generated += 1\n",
    "\n",
    "    new_model_path = f'{BASE_DIR}/{model_name}-v{current_version + 1}'\n",
    "    samplesets = [f'{SAMPLES_DIR}/{i}.json' for i in range(max(0, sample_sets_generated - max_sample_sets), sample_sets_generated)]\n",
    "    train(model_path=current_model_path, sample_paths=samplesets, save_path=new_model_path, size=size, lr=None, epochs=epochs_per_step, optimizer=optimizer)\n",
    "    \n",
    "    # TODO: only replace incumbent if a new version wins >= 55%.\n",
    "    delete_samples_preceeding(sample_sets_generated - max_sample_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [\n",
    "    actor.SFPredictionAgent(\n",
    "        encoder='normalized', \n",
    "        path=f'{model_name}-v{i}',\n",
    "        size=size,\n",
    "        name = f'v{i}',\n",
    "        policy_kind='proportional',\n",
    "    ) for i in range(M)\n",
    "]\n",
    "\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import tournament\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "print(f'PLAYER 1 | PLAYER 2')\n",
    "for (p1, w1), (p2, w2) in tournament.tournament(agents, games.Hex(size=size), 250, verbose=False):\n",
    "    n1, n2 = p1.name, p2.name\n",
    "    # We will always set the lexicographically smallest player first\n",
    "    if n1 <= n2:\n",
    "        key = (n1, n2)\n",
    "        (c, d) = (w1, w2)\n",
    "    else:\n",
    "        key = (n2, n1)\n",
    "        (c, d) = (w2, w1)\n",
    "        \n",
    "    (a, b) = statistics.get(key, (0, 0))\n",
    "    statistics[key] = (a + c, b + d)\n",
    "    \n",
    "    print(f'{p1.name}: {w1:>3}  |  {p2.name}: {w2:>3}')\n",
    "    \n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from itertools import chain\n",
    "from train import load_samples\n",
    "\n",
    "_, B, X, P, Z = (list(chain(*x)) for x in zip(*[\n",
    "    load_samples(f'{SAMPLES_DIR}/{file}') for file in os.listdir(SAMPLES_DIR)\n",
    "]))\n",
    "\n",
    "print(f'Episodes = {sum(1 for (p1, p2) in B if (p1, p2) == (0, 0))}')\n",
    "\n",
    "@interact\n",
    "def show(i = (0, len(B) - 1)):\n",
    "    (p1, p2) = B[i]\n",
    "    return decode_bitboard(p1, p2, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "agent = actor.SFAgent(\n",
    "    leaf_evaluation='rollout', \n",
    "    encoder='normalized', \n",
    "    path=f'{model_name}-v{M}',\n",
    "    c=3,\n",
    "    size=size,\n",
    "    simulations=1000,\n",
    ")\n",
    "\n",
    "interactive.play(games.Hex(size=6), pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "pretrained = actor.SFPredictionAgent(\n",
    "    encoder='normalized', \n",
    "    path=f'/Users/akselborgen/Downloads/oht6x6-v34',\n",
    "    size=6,\n",
    "    name = f'pretrained',\n",
    "    policy_kind='greedy',\n",
    ")\n",
    "\n",
    "interactive.play(games.Hex(size=6), pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHT testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 'oht6x6-resnet128' model used in the OHT (untrained)\n",
    "oht = oht_model(6, residual_blocks=10, softmax_policy=True, convolutional_filter=128, residual_filters=(128, 128))\n",
    "oht.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v34 = actor.SFPredictionAgent(\n",
    "    leaf_evaluation='value_fn', \n",
    "    encoder='normalized', \n",
    "    path=f'/Users/akselborgen/Downloads/oht6x6-v34',\n",
    "    c=3,\n",
    "    size=size,\n",
    "    simulations=1000,\n",
    "    policy_kind='proportional',\n",
    "    name='v34',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v(x):\n",
    "    return actor.SFPredictionAgent(\n",
    "        leaf_evaluation='value_fn', \n",
    "        encoder='normalized', \n",
    "        path=f'/Users/akselborgen/Downloads/oht6x6resnet128-v{x}',\n",
    "        c=3,\n",
    "        size=size,\n",
    "        simulations=500,\n",
    "        policy_kind='proportional',\n",
    "        name=f'r{x:02}',\n",
    "    )\n",
    "\n",
    "def resnet_full_v(x):\n",
    "    return actor.SFAgent(\n",
    "        leaf_evaluation='value_fn', \n",
    "        encoder='normalized', \n",
    "        path=f'/Users/akselborgen/Downloads/oht6x6resnet128-v{x}',\n",
    "        c=3,\n",
    "        size=size,\n",
    "        simulations=500,\n",
    "        name=f'f{x:02}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [resnet_v(0), resnet_v(6), resnet_v(26), resnet_v(41), resnet_v(44)]\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "print(f'PLAYER 1  |  PLAYER 2')\n",
    "for (p1, w1), (p2, w2) in tournament.tournament(agents, games.Hex(size=size), 500, verbose=False):\n",
    "    n1, n2 = p1.name, p2.name\n",
    "    # We will always set the lexicographically smallest player first\n",
    "    if n1 <= n2:\n",
    "        key = (n1, n2)\n",
    "        (c, d) = (w1, w2)\n",
    "    else:\n",
    "        key = (n2, n1)\n",
    "        (c, d) = (w2, w1)\n",
    "        \n",
    "    (a, b) = statistics.get(key, (0, 0))\n",
    "    statistics[key] = (a + c, b + d)\n",
    "    \n",
    "    print(f'{p1.name}: {w1:>3}  |  {p2.name}: {w2:>3}')\n",
    "    \n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "interactive.play(games.Hex(size=size), agent=resnet_full_v(41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, add_batch_axis=False):\n",
    "    tensor = games.hex.normalized_encoder(x)\n",
    "    #tensor = games.hex.current_player_encoder(x)\n",
    "    if add_batch_axis:\n",
    "        return tf.reshape(tensor, (1, *tensor.shape))\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def time_limit(seconds):\n",
    "    start = None\n",
    "    def inner(i):\n",
    "        global start\n",
    "        if i == 0:\n",
    "            start = time.time()\n",
    "        \n",
    "        return (time.time() - start) >= seconds\n",
    "    \n",
    "    return inner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
