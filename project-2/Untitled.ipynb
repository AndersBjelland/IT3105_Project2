{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import games\n",
    "from src import actor\n",
    "from src import interactive \n",
    "from src import tournament\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import choices\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(bitboard, player):\n",
    "    for i in range(64):\n",
    "        bit = bitboard & (0b1 << i)\n",
    "        \n",
    "        x = i % 8\n",
    "        y = i // 8\n",
    "        \n",
    "        if bit != 0: \n",
    "            yield (x, y) if player == 1 else (y, x)\n",
    "            \n",
    "def denormalize_noflip(encoded, size):\n",
    "    current, opponent = encoded[:, :, 0], encoded[:, :, 1]\n",
    "    game = games.Hex(size = size)\n",
    "    game.next_state = \"Disabled\"\n",
    "    for x in range(size):\n",
    "        for y in range(size):\n",
    "            if current[x, y] == 1.0:\n",
    "                game.grid[y][x] = 1\n",
    "            elif opponent[x, y] == 1.0:\n",
    "                game.grid[y][x] = 2\n",
    "    return game\n",
    "\n",
    "            \n",
    "        \n",
    "def interleave(it0, it1):\n",
    "    it0 = iter(it0)\n",
    "    it1 = iter(it1)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            yield next(it0)\n",
    "            yield next(it1)\n",
    "        except StopIteration:\n",
    "            yield from it0\n",
    "            yield from it1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "game = games.Hex(size=6)\n",
    "\n",
    "#for y in range(5):\n",
    "#    game = game.next_state((0, y)).next_state((1, y))\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/Users/akselborgen/Downloads/oht6x6-v34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player, *board = (1, 2, 1, 2, 0, 2, 2, 0, 1, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, 0, 1, 2)\n",
    "player, *board = (1, 2, 2, 2, 0, 0, 2, 1, 1, 1, 2, 0, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 0, 1)\n",
    "#player, *board = (1, 2, 2, 2, 0, 0, 2, 1, 1, 1, 2, 2, 1, 0, 2, 1, 1, 1, 0, 2, 1, 0, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 0, 1)\n",
    "player, *board = (1, 0, 1, 1, 2, 0, 1, 0, 0, 2, 0, 1, 0, 1, 2, 0, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 0, 2, 0, 2)\n",
    "player, *board = (1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "#player, *board = (1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def convolutional_block(x, filters, kernel_size = (3, 3), stride=1, activation=tf.nn.swish):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=kernel_size, strides=(stride, stride))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(x, filters = (256, 256), kernel_size=(3, 3), stride=1, activation = tf.nn.swish):\n",
    "    skip = x\n",
    "    \n",
    "    f1, f2 = filters\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(f1, kernel_size=kernel_size, strides=(stride, stride), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(f2, kernel_size=kernel_size, strides=(stride, stride), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Add()([x, skip])\n",
    "    x = activation(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def body(x, residual_blocks = 1):\n",
    "    x = convolutional_block(x, 256)\n",
    "    \n",
    "    for _ in range(residual_blocks):\n",
    "        x = residual_block(x)\n",
    "        \n",
    "    return x\n",
    "    \n",
    "\n",
    "def policy_head(x, size, activation=tf.nn.swish):\n",
    "    x = tf.keras.layers.Conv2D(2, kernel_size=(1, 1), strides=(1, 1))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(size * size, name='policy')(x)\n",
    "    return x\n",
    "\n",
    "def value_head(x, activation=tf.nn.swish):\n",
    "    x = tf.keras.layers.Conv2D(1, kernel_size=(1, 1), strides=(1, 1))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(256)(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = tf.keras.layers.Activation(tf.nn.tanh, name='value')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def oht_model(size, residual_blocks=1):\n",
    "    input = tf.keras.layers.Input(shape=(size, size, 2), name='input')\n",
    "    \n",
    "    b = body(input, residual_blocks=residual_blocks)\n",
    "    \n",
    "    policy = policy_head(b, size)\n",
    "    value = value_head(b)\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[policy, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = oht_model(6, residual_blocks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('oht6x6-resnet-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(y_true, y_pred):\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    raise ValueError()\n",
    "    \n",
    "def new_policy_loss(y_true, y_pred_before_softmax):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y_true,\n",
    "        logits=y_pred_before_softmax,\n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics={'policy': tf.keras.metrics.CategoricalAccuracy(), 'value': 'mse'},\n",
    "    loss_weights={'value': 1.0, 'policy': 1.0},\n",
    "    loss={'policy': new_policy_loss, 'value': 'mse'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(tf.stack(X[:1000]), y={'policy': tf.stack(P[:1000]), 'value': tf.stack(Z[:1000])}, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(-pi * tf.math.log(tf.math.maximum(pi, 0.00001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, z = model(tf.stack(X)[26:26+1])\n",
    "tf.nn.softmax(p), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(p), np.argmax(P[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0][21], P[0][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(P[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([(x, y) for x in range(6) for y in range(6)], key=lambda t: np.reshape(p, (6, 6))[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(P[0], (6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.reshape(board, (6, 6)).T\n",
    "if False:\n",
    "    grid = grid.T\n",
    "    ones = grid == 1\n",
    "    twos = grid == 2\n",
    "    grid[ones] = 2\n",
    "    grid[twos] = 1\n",
    "    \n",
    "h = games.Hex(size=6)\n",
    "h.grid = grid\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = games.Hex(size = 6)\n",
    "\n",
    "for y in range(5):\n",
    "    g = g.next_state((0, y)).next_state((1, y))\n",
    "g = g.next_state((3, 3))\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(g, add_batch_axis=True)[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(model()[0], (6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = actor.SFAgent(\n",
    "    leaf_evaluation='value_fn',\n",
    "    encoder='normalized',\n",
    "    policy='greedy',\n",
    "    model_path='/Users/akselborgen/Downloads/oht6x6-v34',\n",
    "    size=6,\n",
    "    simulations=10000,\n",
    "    c=3,\n",
    ")\n",
    "\n",
    "agent.policy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(model(encoder(game, add_batch_axis=True))[0], (6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(encoder(game, add_batch_axis=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, z = model(tf.expand_dims(X[2], axis=0))\n",
    "np.reshape(p, (6, 6)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(P[2], (6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import self_play\n",
    "policy, *_ = self_play.policy_distribution(leaf_evaluation='value_fn', encoder='normalized', model_path='/Users/akselborgen/Downloads/oht6x6-v7', size=6, states=[game.grid], simulations=1000, c=3)\n",
    "policy, max(policy, key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = 500\n",
    "policies = [\n",
    "    actor.SFAgent(policy='greedy', leaf_evaluation='value_fn', encoder='normalized', model_path='/Users/akselborgen/Downloads/oht6x6-v34', size=6, simulations=simulations, c=3),\n",
    "    actor.SFAgent(policy='greedy', leaf_evaluation='value_fn', encoder='normalized', model_path='/Users/akselborgen/Downloads/oht6x6-v40', size=6, simulations=simulations, c=3),\n",
    "]\n",
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = []\n",
    "for stats in tournament.tournament(policies, game, 100):\n",
    "    print(stats)\n",
    "    statistics.append(stats)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "interactive.play(game, agent=actor.SFAgent(leaf_evaluation='value_fn', encoder='normalized', model_path='6x6x2', size=6, simulations=10000, c=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Set CPU as available physical device\n",
    "my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')\n",
    "\n",
    "# To find out which devices your operations and tensors are assigned to\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors and perform an operation\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compete(model1, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import self_play\n",
    "dir(self_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import games\n",
    "game = games.Hex(size=5).next_state((0, 0)).next_state((0, 1)).next_state((1, 0)).next_state((1, 1)).next_state((2, 0)).next_state((2, 1)).next_state((3, 0)).next_state((3, 1)).next_state((0, 2))\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import games\n",
    "game = games.Hex(size=6).next_state((3, 1)).next_state((4, 0)).next_state((3, 0)).next_state((0, 4)).next_state((3, 3)).next_state((3, 2)).next_state((2, 2)).next_state((2, 3)).next_state((4, 2)).next_state((2, 4)).next_state((4, 1)).next_state((3, 4)).next_state((4, 3)).next_state((4, 4)).next_state((1, 3)).next_state((1, 4)).next_state((5, 4)).next_state((5, 3))\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "policy, *_ = self_play.policy(leaf_evaluation='rollout', encoder='normalized', model_path='6x6x2', size=6, states=[game.grid], simulations=1000, c=3)\n",
    "policy, max(policy, key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "\n",
    "shape, B, X, P, Z = train.load_samples('/Users/akselborgen/Downloads/21.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out2.txt') as f:\n",
    "    x = f.read()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "! {sys.executable} generate_samples.py --size 5 --model test5x5x2 --out samples/1.json --concurrents 16 --samples 250\n",
    "#! {sys.executable} train.py --size 5 --model test5x5x2 --out test5x5x2v1 --data samples/1.json --lr 0.2 --epochs 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = actor.BetaHex(\n",
    "    size = 5,\n",
    "    encoder = encoder,\n",
    "    shape = encoder(game).shape,\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    ")\n",
    "\n",
    "#@tf.function\n",
    "#def policy_loss(y_true, y_pred):\n",
    "#    return tf.reduce_mean(tf.reduce_sum(- y_true * tf.math.log(tf.math.maximum(y_pred, 0.0001)), axis=[1]))\n",
    "\n",
    "#tf.keras.losses.policy_loss = policy_loss\n",
    "\n",
    "#policy.compile(\n",
    "#    optimizer=tf.keras.optimizers.Adam(), \n",
    "#    metrics=['accuracy'],\n",
    "#    loss_weights={'output_1': 1.0, 'output_2': 1.0},\n",
    "#    loss = {'policy': policy_loss, 'value': 'mse'},\n",
    "#)\n",
    "# To force the tensorflow model to be built\n",
    "policy.model.set_weights(tf.keras.models.load_model('test5x5x2').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, z = policy.model(tf.expand_dims(X[180], axis=0))\n",
    "np.array(p).reshape((5, 5)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(P[180]).reshape((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 250\n",
    "policy.model.fit(tf.stack(X[:N]), y={'policy': tf.stack(P[:N]), 'value': tf.stack(Z[:N])}, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "self_play.run_save(model_path='test5x5x2', size=5, concurrents=1, simulations=100, samples=100, leaf_evaluation='value_fn', encoder='normalized', out_path='samples/1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('samples/1.json', 'r') as f:\n",
    "    js = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_samples(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    shape = data['stateShape']\n",
    "    samples = data['samples']\n",
    "\n",
    "    X = []\n",
    "    P = []\n",
    "    Z = []\n",
    "\n",
    "    for sample in samples:\n",
    "        X.append(np.array(sample['state']).reshape(shape))\n",
    "        P.append(np.array(sample['policy']))\n",
    "        Z.append(sample['value'])\n",
    "\n",
    "    return shape, X, P, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape, X, P, Z = load_samples('samples/1.json')\n",
    "\n",
    "x = [tuple(x.reshape((-1,))) for x in X]\n",
    "I = [i for i, a in enumerate(x) if a == x[9]]\n",
    "len(x), len(set(x)), I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[I[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([a for a in x if sum(1 for b in x) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "self_play.run(path='test5x5x2', size=5, concurrents=1, simulations=100, samples=100, leaf_evaluation='rollout', encoder='normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm \"self-play.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_play.init_logging('debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for c in [128, 256, 512, 1024, 2048]:\n",
    "    N = 24 * c\n",
    "    print(f'{c} concurrents')\n",
    "    print(f'{N} samples to generate')\n",
    "    start = time.time()\n",
    "    self_play.run(path='test5x5x2', size=5, concurrents=c, simulations=100, samples=N)\n",
    "    end = time.time()\n",
    "    throughput = N / (end - start)\n",
    "    print(f'{end - start} s')\n",
    "    print(f'{throughput} samples/s')\n",
    "    print(flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse(xs):\n",
    "    lines = [line.split(' ')[0] for line in xs.splitlines()]\n",
    "    concurrents, samples, time, throughput = lines[0::5], lines[1::5], lines[2::5], lines[3::5]\n",
    "    \n",
    "    for (c, s, t, thr) in zip(concurrents, samples, time, throughput):\n",
    "        yield (int(c), int(s), float(t), float(thr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = list(parse(log))\n",
    "\n",
    "C = [t[0] for t in samples]\n",
    "T = np.array([t[3] for t in samples])\n",
    "\n",
    "plt.plot(C, T / T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(xs) / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def f():\n",
    "    xs = self_play.run(path='test5x5x2', size=5, concurrents=1, simulations=100, samples=110)\n",
    "    print(xs, flush=True)\n",
    "    return all(x >= 5 for x in xs)\n",
    "    \n",
    "\n",
    "\n",
    "while f():\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "p1 = 112189247232\n",
    "p2 = 30133257493\n",
    "\n",
    "print(len(list(decode(p1, 1))), len(list(decode(p2, 2))))\n",
    "\n",
    "game = games.Hex(size = 5)\n",
    "print(game.winner())\n",
    "for action in interleave(decode(p1, 1), decode(p2, 2)):\n",
    "    game = game.next_state(action)\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 1\n",
    "N = 50000\n",
    "C = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "for concurrents in C:\n",
    "    %time generated = self_play.run(path='test5x5x2', size=5, concurrents=concurrents, simulations=S, samples=N)\n",
    "    print(f\"{concurrents}: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 68920804608\n",
    "p2 = 16843009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(decode(p1, 1))), len(list(decode(p2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "game = games.Hex(size = 5)\n",
    "print(game.winner())\n",
    "for action in interleave(decode(p1, 1), decode(p2, 2)):\n",
    "    game = game.next_state(action)\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_eight = 0b11111111   \n",
    "max_size = 8\n",
    "board = p2\n",
    "#// Both boards have the same \"perspective\" as that of player 1, and can be treated equivalently\n",
    "reachable = [0 for _ in range(5 + 1)]\n",
    "reachable[0] = board & lower_eight\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    for i in range(1, 5):\n",
    "        print(f'i = {i}')\n",
    "        print(f'\\tingoing = {[f\"{r:05b}\" for r in reachable]}')\n",
    "        row = (board >> (max_size * i)) & lower_eight\n",
    "        print(f'\\trow     = {row:5b}')\n",
    "        reachable[i] = row & (reachable[i - 1] >> 1 | reachable[i - 1] | reachable[i + 1] << 1 | reachable[i + 1] | reachable[i] >> 1 | reachable[i] << 1)\n",
    "        \n",
    "[f\"{r:05b}\" for r in reachable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, add_batch_axis=False):\n",
    "    tensor = games.hex.normalized_encoder(x)\n",
    "    #tensor = games.hex.current_player_encoder(x)\n",
    "    if add_batch_axis:\n",
    "        return tf.reshape(tensor, (1, *tensor.shape))\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def time_limit(seconds):\n",
    "    start = None\n",
    "    def inner(i):\n",
    "        global start\n",
    "        if i == 0:\n",
    "            start = time.time()\n",
    "        \n",
    "        return (time.time() - start) >= seconds\n",
    "    \n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "game = games.Hex(size = 6)\n",
    "print(game.winner())\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = actor.BetaHex(\n",
    "    size = 6,\n",
    "    encoder = encoder,\n",
    "    shape = encoder(game).shape,\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    ")\n",
    "\n",
    "#@tf.function\n",
    "#def policy_loss(y_true, y_pred):\n",
    "#    return tf.reduce_mean(tf.reduce_sum(- y_true * tf.math.log(tf.math.maximum(y_pred, 0.0001)), axis=[1]))\n",
    "\n",
    "#tf.keras.losses.policy_loss = policy_loss\n",
    "\n",
    "#policy.compile(\n",
    "#    optimizer=tf.keras.optimizers.Adam(), \n",
    "#    metrics=['accuracy'],\n",
    "#    loss_weights={'output_1': 1.0, 'output_2': 1.0},\n",
    "#    loss = {'policy': policy_loss, 'value': 'mse'},\n",
    "#)\n",
    "# To force the tensorflow model to be built\n",
    "policy(tf.expand_dims(encoder(game), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.model.save('6x6x2/', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy.body.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.policy_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.value_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts = actor.MCTS(\n",
    "    default_policy = policy,\n",
    "    leaf_evaluator = actor.Rollout(policy),\n",
    "    terminate = lambda i: i >= 10,\n",
    "    c = 0.4,\n",
    ")\n",
    "\n",
    "raw_replay_buffer = []\n",
    "replay_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "interactive.play(game, mcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "#gen = mcts.generate_episodes(game, concurrents=32)\n",
    "for i in tqdm(range(1)):\n",
    "    for _ in tqdm(range(1)):\n",
    "        samples = mcts.episode(game)\n",
    "        raw_replay_buffer.extend(samples)\n",
    "        replay_buffer.extend((policy.encoder(s), (policy.numpy_distribution(D), z)) for (s, D, z) in samples)\n",
    "        \n",
    "        X, Y = zip(*choices(replay_buffer, k=100))\n",
    "        X = tf.stack(X)\n",
    "        P = tf.stack([p for p, _ in Y])\n",
    "        Z = tf.expand_dims(tf.stack([float(z) for _, z in Y]), axis=1)\n",
    "        histories.append(policy.model.fit(X, y = {'policy': P, 'value': Z}, epochs=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = zip(*choices(replay_buffer, k=100))\n",
    "X = tf.stack(X)\n",
    "P = tf.stack([p for p, _ in Y])\n",
    "Z = tf.expand_dims(tf.stack([float(z) for _, z in Y]), axis=1)\n",
    "histories.append(policy.model.fit(X, y = {'policy': P, 'value': Z}, epochs=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = zip(*processed_replay_buffer[:2])\n",
    "#X = tf.stack(X)\n",
    "#P = tf.stack([p for p, _ in Y])\n",
    "#Z = tf.expand_dims(tf.stack([float(z) for _, z in Y]), axis=1)\n",
    "def expand_sample(d):\n",
    "    keys, values = zip(*d.items())\n",
    "    \n",
    "    for sample in zip(*values):\n",
    "        yield {\n",
    "            k: v for k, v in zip(keys, sample)\n",
    "        }\n",
    "    \n",
    "\n",
    "#df = pd.DataFrame(policy.fit(X, y = {'output_1': P, 'output_2': Z}, epochs=1000).history)\n",
    "df = pd.DataFrame([d for h in histories for d in expand_sample(h.history)])\n",
    "#df = pd.DataFrame([{k: sum(v) / len(v) for k, v in h.history.items()} for h in histories])\n",
    "\n",
    "for column in df.columns:\n",
    "    plt.title(column)\n",
    "    plt.plot(df[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(i):\n",
    "    dumb_policy = actor.NeuralNetworkActorCritic(\n",
    "        encoder = encoder,\n",
    "        epsilon=0.0,\n",
    "    )\n",
    "\n",
    "    dumb_policy.model.load_weights(f'weights{i}')\n",
    "\n",
    "    return actor.MCTS(\n",
    "        default_policy = dumb_policy,\n",
    "        terminate = time_limit(2)\n",
    "    )\n",
    "\n",
    "agents = [(i, checkpoint(i)) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "def compete(agent1, agent2, times, state):\n",
    "    initial_state = state\n",
    "    agents = [agent1, agent2]\n",
    "    stats = [0, 0]\n",
    "    \n",
    "    for _ in tqdm(range(times)):\n",
    "        # We will let both players play `times` times as the first one to make a move.\n",
    "        order = [[0, 1], [1, 0]]\n",
    "        for o in order:\n",
    "            state = initial_state\n",
    "            for player in cycle(o):\n",
    "                winner = state.winner()\n",
    "                if winner is not None:\n",
    "                    stats[o[winner - 1]] += 1\n",
    "                    break\n",
    "                    \n",
    "                action = agents[player].policy(state)\n",
    "                state = state.next_state(action)\n",
    "    \n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "for (i0, a0), (i1, a1) in combinations(agents, r=2):\n",
    "    (w0, w1) = compete(a0, a1, 10, game)\n",
    "    print(f'{i0} wins {w0}; {i1} wins {w1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game\n",
    "history = [state]\n",
    "while not state.is_final():\n",
    "    action = mcts.policy(state)\n",
    "    state = state.next_state(action)\n",
    "    history.append(state)\n",
    "    \n",
    "@interact\n",
    "def show(i = (0, len(history) - 1, 1)):\n",
    "    return history[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
